{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Predictably Comforting:<br>The Writings of Soo Ewe Jin\"\nauthor: \"Kevin W. Soo\"\ndate: \"November 22, 2016\"\noutput:\n  html_document:\n    toc: yes\n    toc_depth: 6\n    toc_float: no\n  pdf_document:\n    toc: yes\n    toc_depth: '6'\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n\n# load packages\nlibrary(tidyverse)\nlibrary(reshape2)\nlibrary(ggthemes)\nlibrary(tm)\nlibrary(SnowballC)\nlibrary(wordcloud)\nlibrary(cluster)\nlibrary(fpc)\n\n# load data\nload(file=\"ejStar.Rda\")\n\n######### pre-processing text\n\n# create corpus\ncorpus <- Corpus(VectorSource(ej$text))\n\n# preprocessing\ncorpus <- tm_map(corpus, removePunctuation) \ncorpus <- tm_map(corpus, removeNumbers)\ncorpus <- tm_map(corpus, tolower)\n# inspect(corpus)[[1]][1]\n\n# remove stopwords\ncorpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\nminorwords <- c(\"soo\", \"ewe\", \"jin\", \"executive\", \"editor\", \"Ã¢\") # remove specific words here\ncorpus <- tm_map(corpus, removeWords, minorwords)\n\n# remove stems and whitespace\ncorpus <- tm_map(corpus, stemDocument)\ncorpus <- tm_map(corpus, stripWhitespace)\n\n# save as text document\ncorpus <- tm_map(corpus, PlainTextDocument)\n\n# create document term matrix\ndtm <- DocumentTermMatrix(corpus)\ntdm <- TermDocumentMatrix(corpus)\n\n########## reshape data for plotting\n\n# word frequencies\nfreq <- colSums(as.matrix(dtm))   \n\n# remove sparse words\ndtms <- removeSparseTerms(dtm, 0.5) # 50% empty space, maximum\nrownames(dtms) <- ej$title # document titles\n\n# data frame of frequencies\nwf <- data.frame(word=names(freq), freq=freq) %>% arrange(-freq)\nwf$rank <- 1:nrow(wf)\nwf$word <- factor(wf$word, levels=wf[order(wf$rank),\"word\"])\n\n# set up family data frame\nfamily <- wf %>% filter(word %in% c(\"kevin\", \"tim\", \"son\", \"wife\", \"angie\", \"evelyn\"))\n\n# create dendrogram\nd <- dist(t(dtms), method=\"euclidian\")   \nfit <- hclust(d=d, method=\"ward.D\")  \n\n# data frame of distances\nmeltedWords <- data.frame(melt(as.matrix(d)))\ncolnames(meltedWords) <- c(\"i\", \"j\", \"Distance\")\n\n# load topic modeling data\nload(file=\"topics.Rda\")\ntopics$section <- ej$category\n# topics in long format\ntopicsOverTime <- topics %>% gather(topics, p, Encouragement:Society) %>% arrange(num, topics)\n```\n\n***\n\n### 1 Remembering Soo Ewe Jin\n\nMy father passed away on November 17, 2016. His life has been memorialized in [countless](http://www.thestar.com.my/news/nation/2016/11/17/veteran-the-star-editor-soo-ewe-jin-passes-away/) [tributes](http://www.thestar.com.my/opinion/columnists/sunday-starters/2016/11/20/continue-his-legacy-of-kindness-may-the-spirit-of-this-column-live-on-and-may-we-all-keep-on-buildin/) [published](http://www.thestar.com.my/opinion/columnists/on-the-beat/2016/11/20/farewell-my-dear-friend-ewe-jin-he-was-one-of-the-kindest-most-generous-and-positive-people-i-have-h/) in the [Malaysian](http://www.thestar.com.my/news/nation/2016/11/20/sundays-will-never-be-the-same-again/) [press](http://www.thestar.com.my/opinion/letters/2016/11/21/make-sure-each-moment-and-each-day-counts/) and on social media. An executive editor at the most widely-read English daily in Malaysia, he had a popular weekly column: [*Sunday Starters*](http://www.thestar.com.my/search/?q=%22sunday+starters%22&qkey=sunday+starters).\n\nAs a way of navigating through the mourning process, I set out to collect all his writings published in *The Star*. I'm sure the editors would have given me all his writings if I had asked, but instead I wrote some [code]() to scrape all 366 articles he authored from the website. To try this yourself, run `scrape_articles.R`, which retrieves all his articles and saves them into a data frame.\n\nPlenty of people found my dad's writing to be encouraging and comforting -- I've been told that it fell within the 'chicken-soup-for-the-soul' genre of writing. While that is a decent general description of his body of work, I sought a more satisfactory data-driven description.\n\n### 2 In his own words\n\nWords are a writer's tools, used to convey information and elicit responses in readers. The first part of this analysis looks at the words my dad used in his writing. Perhaps the words he used can shed light on the kinds of concepts he was most interested in communicating to readers.\n\n#### 2.1 Who was his favorite?\n\nMy dad was fond of writing about his family, though we hated when we became unwitting subjects of the week's column. He usually anonymized our appearances as requested, but sometimes mentioned us anyway. The following graph displays the number of appearances of family members (both in name and title).\n\n```{r family, echo=FALSE, fig.align=\"left\", fig.width=10, fig.height=3.5}\nfamily %>%\n    ggplot(aes(x=word, y=freq)) +\n    geom_bar(stat=\"identity\", aes(fill=word)) +\n    geom_label(aes(label=freq, x=word)) +\n    coord_flip() +\n    scale_fill_ptol() +\n    theme_minimal() +\n    theme(legend.position=\"none\") +\n    xlab(\"Family member\") +\n    ylab(\"Frequency\") +\n    ggtitle(\"Most frequently mentioned family members by Soo Ewe Jin\")\n```\n\nMy wife ('Evelyn') got one mention in my dad's last few columns. He mentions 'son' 76 times, but this doesn't discriminate between mentions of Tim (my brother) and I. I am mentioned by name twice, compared to Tim who is mentioned 5 times. All these mentions pale in comparison to my mother -- she is mentioned by name ('Angie') once, but referred to as his wife 136 times.\n\n#### 2.2 Frequently-used words\n\nA crude way of determining what he wrote about is by looking at his most frequently-used words. The following graph plots the frequencies of the 25 most common words in his writing (excluding stopwords like 'a', 'the', etc.). \n\n```{r histogram, echo=FALSE, fig.align=\"center\", fig.width=10}\n# plot word frequencies\nwf %>% filter(rank<=25) %>%\n    ggplot(aes(x=word, y=freq)) +\n    geom_bar(stat=\"identity\", aes(fill=freq)) +\n    scale_color_ptol(\"cyl\") +\n    theme_minimal() +\n    theme(axis.text.x=element_text(angle=45, hjust=1)) +\n    xlab(\"Words\") +\n    ylab(\"Frequency\") +\n    labs(fill='Frequency') +\n    ggtitle(\"Most frequently used words by Soo Ewe Jin\")\n```\n\nSome of these high-frequency words give us clues about his subject-matter: 'people' (857 appearances), 'time' (696 appearances), and 'life' (561 appearances) fit pretty well on a conceptual level with the focus of his column -- reflections about appreciating people and everyday things. Since the text I analyzed consisted of 366 articles, most of these words appeared more than once per article (on average).\n\nVisualizing these in a wordcloud gives us more room, so we can look at the 100 most frequently used words (each of which appears > 130 times in his writings). The size and color of each word corresponds to how frequently it appears in his writing.\n```{r wordcloud, echo=FALSE, fig.align=\"center\", fig.width=7}\n# plot wordcloud\nwordcloud(names(freq), freq, max.word=100, # top 100 words\n          scale=c(4,.05), rot.per = .25,\n          colors=brewer.pal(9, \"PuBuGn\")) # number of colors, palette\n```\nIn the wordcloud, you'll see other semi-frequent words like 'work' (423 appearances), 'world' (392 appearances), and 'friends' (292 appearances) that give you more hints of what he often wrote about. However, word-frequency alone doesn't paint a very clear picture. There are some high-frequency words like 'will' (1012 appearances), 'can' (920 appearances), and 'one' (831 appearances) that don't suggest particular subject-matter and are highly dependent on context.\n\nA bigger problem is that we're assuming particular nouns work as standalone markers and evidence of particular topics. However, the absence of a word (e.g., 'friends') doesn't necessarily mean that a particular topics isn't being written about. The following analyses look beyond individual words to the relationships between words.\n\n#### 2.3 Word neighbors\n\nA word's immediate context may give us an idea of what it means. If a word like 'will' often appears near 'friends', then the writer is probably using 'will' to write about the things that friends tend to do, rather than using it in reference to a legal document. Restricting the analysis to the 25 most frequently-used words, we can calculate the [euclidian distance](https://en.wikipedia.org/wiki/Euclidean_distance) between each pair of words. These distances are plotted in the following heatmap, with darker colors representing pairs that appear closer together on average. The distance between each word with itself, which is 0, is represented on the diagonal.\n\n```{r heatmap, echo=FALSE, fig.align=\"center\", fig.width=7}\n# plot word frequencies\nmeltedWords %>% arrange(-Distance) %>%\n    ggplot(aes(x=i, y=j, fill=Distance)) +\n    geom_raster() +\n    theme_minimal() +\n    theme(axis.text.x=element_text(angle=45, hjust=1)) +\n    xlab(\"\") +\n    ylab(\"\") +\n    labs(fill='Distance') +\n    ggtitle(\"Euclidian distance between frequently used words\")\n```\n\nUnfortunately, it's hard to decipher any meaningful pattern in the distance between pairs of words from the heatmap. From the computed distances, we can generate a dendrogram (a type of [tree graph](https://en.wikipedia.org/wiki/Tree_(graph_theory))) to identify pairs of words that are *neighbors* of each other.\n\nThis dendrogram organizes the 25 most frequently-used words by their distance to each other, revealing \"neighborhoods\" of words. This highlights words that are frequently used in tandem, suggesting either that they are parts of commonly-used phrases (e.g., 'one time'), or that they are concepts the writer treated as related (e.g., 'work' and 'life').\n```{r dendrogram, echo=FALSE, fig.align=\"center\", fig.width=7}\nplot(fit, hang=-1, main=\"Neighboring words\", xlab=\"\", ylab=\"\", axes=F, sub=\"\")\n```\nMoving down the tree, each branching creates two neighborhoods, showing us which word neighborhoods are close to each other. At the lowest level, we can see which individual words are neighbors with each other.\n\nFrom the top-down, the first level of branching produces one large and diverse neighborhood on the left, and one smaller but more focused neighborhood on the right. This latter neighborhood seems to represent a cluster of words used in tandem with 'people', suggesting my dad wrote a lot about people using these words.\n\nI've not attempted an exhaustive analysis of the words in my dad's writings. People are free to infer how these words all relate to each other and what they imply about the subject-matter of his writings. My goal in this section was simply to compute these basic statistics and visualize them to memorialize the words he used. The following analyses move beyond surface-level words to focus on deeper information -- the topics -- contained in his writings.\n\n### 3 Underlying topics\n\nAs mentioned above, the absence of a particular word doesn't imply that a particular topic isn't being written about. In this section, I attempt to build a [topic model](https://en.wikipedia.org/wiki/Topic_model) of my dad's writings. A topic model assumes:\n\n1. *In a collection of writings by a single writer, there will be a number of topics being written about.* The data here consists of 366 articles by my dad. There will be more than one topic because the articles are not all about the same thing, but there will not be 366 topics because multiple articles will be about the same topic. Every writer probably writes reliably about a small number of topics (we'll say a columnist like my dad probably wrote about 2--5 topics throughout his writings).\n\n2. *Each topic is written about using reliable collections of words.* When writing about a particular topic, a writer tends to use the same words. These words need not contain an explicit label for the topic (e.g., a topic about 'sports' need not contain any names of sports; they could just contain verbs like 'run', 'kick', and 'jump'). A topic is thus a cluster of co-occuring words that are linked by some underlying generative process -- a writer \"generates\" these words when communicating a particular topic.\n\n3. *Each article is about a combination of the topics in different proportions; we figure out the proportions from the words that are present.* Some articles are about a particular topic more than any other -- if so, we expect to see many words associated that topic, but few words associated with other topics. Some articles are about multiple topics, in which case we expect to see words associated with multiple topics.\n\n#### 3.1 Identifying the topics\n\nIt's tricky to identify the number of topics ($k$) present in a corpus. There are some [mathematical](http://stackoverflow.com/questions/21355156/topic-models-cross-validation-with-loglikelihood-or-perplexity/21394092) [approaches](http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-16-S13-S8) to determining the optimal $k$, but they do not always produce topics (i.e. clusters of words) that are semantically meaningful. A simpler method is specifying an *a priori* number of topics, running the algorithm, and looking at the words (terms) associated with each topic to see if they are semantically meaningful. As mentioned above, $k$ of 2 to 5 seem like reasonable values for a columnist like my dad.\n\nUsing the `topicmodels` package, I ran the Latent Dirichlet allocation (LDA) algorithm with $k = 2, 3, 4,$ and $5$, and determined that having three topics produced the best (most meaningful) topic distribution. The table below shows the three topics, which I have named, along with the top five terms associated with each.\n\nEncouragement | Reflections  | Society\n--------------|--------------|--------------\ncan           | life         | work\npeople        | time         | public\nwill          | way          | Malaysia\nstill         | day          | country\nworld         | one          | first\n\nThe *Encouragement* topic probably encompasses writings of my dad where he gave exhortations to his readers to positively influence the world. *Reflections* probably include musings about aspects of daily life and the passage of time. Articles about *Society* are about the workplace and aspects of Malaysian society.\n\n#### 3.2 Classifying articles\n\nThe following graph displays the classification of all the articles based on their dominant topics.\n\n```{r classification, echo=FALSE, fig.align=\"left\", fig.width=10, fig.height=2}\ntopics %>% group_by(topic) %>% summarise(N=n()) %>%\n    ggplot(aes(x=topic, y=N)) +\n    geom_bar(stat=\"identity\", aes(fill=topic)) +\n    geom_label(aes(label=N, x=topic)) +\n    coord_flip() +\n    scale_fill_ptol(\"Topic\") +\n    theme_minimal() +\n    theme(legend.position=\"none\") +\n    xlab(\"Topic\") +\n    ggtitle(\"Classification of articles by topic\")\n```\n\nI simply categorized each article based on which topic was most present. However, as noted above, multiple topics can be present in different proportions in a given article. Therefore, classifying articles in this way loses significant information about the mixture of topics.\n\nFor each article, LDA actually gives us the topic distribution, $P(topic|article)$, such that the mixture of all topics sums to 1 within each article. An article classified as *Encouragement* above may have gotten that classification despite all topics being somewhat equally present, but with $P(topic = Encouragement|article)$ being slightly higher than the second-most present topic. For a fuller picture, the following graph plots the topic distributions for all my dad's articles.\n\n```{r topicsOverTime, echo=FALSE, fig.align=\"left\", fig.width=10}\ntopicsOverTime %>% filter() %>%\n    ggplot(aes(x=num, y=p, group=topics)) + \n    geom_bar(stat=\"identity\", width=0.9, aes(fill=topics)) +\n    scale_fill_ptol(\"Topic\") +\n    theme_minimal() +\n    xlab(\"Article #\") +\n    ylab(\"P(topic|article)\") +\n    ggtitle(\"Topic distributions for all articles\")\n```\n\nAs can be seen, my dad wrote pretty evenly about all three topics in all his articles. However, it might be interesting to look at articles where one topic was significantly more present than the others, to see how well the topic model did. For each topic, I looked for one article where the topic distribution heavily favored the main topic relative to the other two. The following table displays these articles and $P(topic|article)$ for each topic (the probability of the main topic is **bolded**).\n\nArticle | Encouragement | Reflections | Society\n--------|---------------|-------------|--------\n[*Repair -- the fourth 'R' missing in our lives*](http://www.thestar.com.my/business/business-news/2011/02/28/repair--the-fourth-r-missing-in-our-lives/)| **.58** | .21 | .21\n[*My silver lining*](http://www.thestar.com.my/opinion/columnists/sunday-starters/2014/06/08/my-silver-lining-when-people-relate-to-my-journeys-with-cancer-they-often-focus-on-the-ailment-rathe/)| .15 | **.70** | .15\n[*A time for reflection*](http://www.thestar.com.my/news/nation/2014/09/28/a-time-for-reflection-the-law-fraternity-will-honour-the-late-sultan-of-perak-at-this-years-sultan-a/)| .09 | .09 | **.82**\n\nFrom reading the articles, it might be difficult to distinguish between *Encouragement* and *Reflections*. This is because all articles contain some mixture of the two, and because there is probably some conceptual overlap between these topics. \n\nAn interesting result is that articles about *Society* tended to be those that were not from his weekly column -- they tended to be profiles, interviews, and obituaries of prominent figures in Malaysia's civil society and business world. Most of these articles were found in the business section of the newspaper -- where he also contributed some writings.(It is also worth noting that the article above about *Society* is titled *A time for reflection*, but is actually an article reflecting on the life of a prominent member of Malaysia's legal fraternity that doesn't contain the type of reflection typical of his *Reflections*).\n\nWe expect articles in the business section to be about *Society* much more than we expect them to be *Encouragement* or *Reflections*. I did a quick analysis to see how many of his articles from the business section (52 of them) the LDA process correctly classified as being about the *Society* topic.\n\n```{r business, echo=FALSE, fig.align=\"left\", fig.width=10, fig.height=2}\ntopics %>% filter(section==\"Business\") %>% group_by(section, topic) %>% summarise(N=n()) %>%\n    ggplot(aes(x=topic, y=N)) +\n    geom_bar(stat=\"identity\", aes(fill=topic)) +\n    geom_label(aes(label=N, x=topic)) +\n    coord_flip() +\n    scale_fill_ptol(\"Topic\") +\n    theme_minimal() +\n    theme(legend.position=\"none\") +\n    xlab(\"Topic\") +\n    ggtitle(\"Classification of business section articles\")\n```\n\nOnly 22 of 52 articles he wrote in the business section were classified as being about *Society*. It seems that even when writing in the business section, he could not help but include *Encouragement* and *Reflections*. That's just the kind of person he was.\n\n#### 3.3 Writing over time\n\nThe prior analysis shows my dad's writings were pretty consistently about three dominant topics. Did his focus on each of these topics change over time? The following graphs plot $P(topic|article)$ for each topic over time. A linear trend has been fitted to each graph so we can see if there are trends in each topic.\n\n```{r trends, echo=FALSE, fig.align=\"left\", fig.width=10}\ntopicsOverTime %>% filter() %>%\n    ggplot(aes(x=date, y=p, group=topics)) + \n    facet_grid(~ topics) +\n    geom_point(stat=\"identity\", aes(color=topics), alpha=0.4) +\n    stat_smooth(color=\"gray25\", method=lm) +\n    scale_color_ptol(\"Topic\") +\n    theme_minimal() +\n    theme(legend.position=\"none\") +\n    scale_x_date(date_breaks = \"1 year\", date_minor_breaks = \"6 months\", date_labels = \"%Y\") +\n    theme(axis.text.x  = element_text(angle=45, vjust=0.5)) +\n    xlab(\"Date\") +\n    ylab(\"P(topic|article)\") +\n    ggtitle(\"Trends in topics over time\")\n```\n\n*Society* generally had a smaller presence in most of his writings -- there seems to be the occasional article that is strongly about the topic, while it is present only at a low level for the majority of his writings. My dad occasionally wrote articles for the business and news sections of the paper, which probably accounted for the articles where $P(topic=Society|article)$ are high. He worked in the business section in his earlier years, but stopped writing for them later on -- which appears to be reflected in the overall decrease in this topic's presence in his writings.\n\nOver time, his writings increasingly focused on *Reflections* and less on *Encouragement* (the correlation between these topics is negative, $r = -.59$). This doesn't mean my dad was getting less encouraging, merely that over time his writings consisted less of words exclusive to that topic (and even in his last few years, $P(topic=Encouragement|article)$ was still pretty high -- around .3 on average). My best guess is that my dad became more reflective (see the increase in *Reflections* over time), and that decreased the space he could dedicate to other topics.\n\n### 4 Conclusion\n\nAs much as an analysis of word frequencies and topic models can shed light on the content of my dad's writings, they cannot capture everything. This failure goes beyond the deliberate simplification of problems I have permitted to keep things practical, and beyond the subjective nature of some of these analyses (e.g., I don't have any more justification for choosing $k = 3$ other than that those three topics make sense). \n\n**The analyses fail because my dad was much more than just his writings. I do not present any analyses that allow us to make inferences about the character of this man.**\n\nWhat I am doing is inherently reductionistic -- I want to summarize his writings to make sense of them. My dad loved writing, so this is an attempt to preserve that which he loved. My dad wrote about matters of the heart, and probably would not have bought fully into the data-driven approach I have used.\n\n*This post is dedicated to Soo Ewe Jin (1959--2016).*",
    "created" : 1481410565125.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1088470448",
    "id" : "14248181",
    "lastKnownWriteTime" : 1480557821,
    "last_content_update" : 1480557821,
    "path" : "~/Box Sync/Personal/rememberingDad/PredictablyComforting.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}